{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING]: failed to patch stdout/stderr for fork-safety: 'OutStream' object\n",
      "has no attribute 'buffer'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "\n",
    "import bittensor\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import datasets\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from accelerate import Accelerator\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cfg_and_load_defaults(cfg: DictConfig) -> DictConfig:\n",
    "\n",
    "    subtensor = bittensor.subtensor(network=cfg.bittensor.network)\n",
    "    if cfg.dataset.block_size is None:\n",
    "        cfg.dataset.block_size = subtensor.validator_sequence_length\n",
    "    if cfg.training.train_batch_size is None:\n",
    "        cfg.training.train_batch_size = subtensor.validator_batch_size\n",
    "    if cfg.training.eval_batch_size is None:\n",
    "        cfg.training.eval_batch_size = subtensor.validator_batch_size\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(cfg: DictConfig):\n",
    "    status = True\n",
    "    i = 0\n",
    "\n",
    "    while status:\n",
    "        file_name = cfg.dataset.file_name + \"_\" + str(i)\n",
    "        data_file = os.path.join(cfg.dataset.data_dir, file_name)\n",
    "\n",
    "        try:\n",
    "            tokenized_dataset_batch = load_from_disk(data_file)\n",
    "            tokenized_dataset_batch.set_format(type='pt')\n",
    "            print(f\"loaded data from {data_file}.\")\n",
    "        except:\n",
    "            status = False\n",
    "            print(f\"{data_file} doesn't exist.\")\n",
    "\n",
    "        if i==0:\n",
    "            tokenized_dataset = tokenized_dataset_batch\n",
    "        else:\n",
    "            tokenized_dataset = concatenate_datasets([tokenized_dataset, tokenized_dataset_batch])\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(cfg: DictConfig):\n",
    "    if cfg.tokenizer.name is not None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            cfg.tokenizer.name, use_fast=cfg.tokenizer.use_fast\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            cfg.model.name, use_fast=cfg.tokenizer.use_fast\n",
    "        )\n",
    "    \n",
    "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accelerate config file at: /home/paperspace/.cache/huggingface/accelerate/default_config.yaml\n",
    "\n",
    "# compute_environment: LOCAL_MACHINE\n",
    "# distributed_type: MULTI_GPU\n",
    "# downcast_bf16: 'no'\n",
    "# gpu_ids: all\n",
    "# machine_rank: 0\n",
    "# main_process_ip: ''\n",
    "# main_process_port: 29500\n",
    "# main_training_function: main\n",
    "# mixed_precision: 'no'\n",
    "# num_machines: 4\n",
    "# num_processes: 4\n",
    "# rdzv_backend: static\n",
    "# same_network: true\n",
    "# tpu_env: []\n",
    "# tpu_use_cluster: false\n",
    "# tpu_use_sudo: false\n",
    "# use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accelerate\n",
    "\n",
    "# accelerate.load_state(input_dir=\"/home/paperspace/.cache/huggingface/accelerate/default_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_accelerator(cfg: DictConfig) -> Accelerator:\n",
    "\n",
    "    accelerator = (\n",
    "        Accelerator(log_with=cfg.tracking.report_to, project_dir=cfg.output_dir)\n",
    "        if cfg.tracking.enabled\n",
    "        else Accelerator()\n",
    "    )\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    return accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(cfg: DictConfig, tokenizer):\n",
    "\n",
    "    if cfg.model.config_name is not None:\n",
    "        config = AutoConfig.from_pretrained(cfg.model.config_name)\n",
    "    else:\n",
    "        config = AutoConfig.from_pretrained(cfg.model.name)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model.name,\n",
    "        from_tf=bool(\".ckpt\" in cfg.model.name),\n",
    "        config=config,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(cfg, model):\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": cfg.training.weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    return torch.optim.AdamW(\n",
    "        optimizer_grouped_parameters, lr=cfg.training.learning_rate\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=17):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/share/ipfs-data/data/tokenized_data_0 doesn't exist.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_18277/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">4158137093.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">9</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_18277/4158137093.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_18277/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">1169894073.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">17</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_dataset</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_18277/1169894073.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">UnboundLocalError: </span>local variable <span style=\"color: #008000; text-decoration-color: #008000\">'tokenized_dataset_batch'</span> referenced before assignment\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_18277/\u001b[0m\u001b[1;33m4158137093.py\u001b[0m:\u001b[94m9\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_18277/4158137093.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_18277/\u001b[0m\u001b[1;33m1169894073.py\u001b[0m:\u001b[94m17\u001b[0m in \u001b[92mload_dataset\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_18277/1169894073.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mUnboundLocalError: \u001b[0mlocal variable \u001b[32m'tokenized_dataset_batch'\u001b[0m referenced before assignment\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print('a')\n",
    "# accelerator = create_accelerator(cfg)\n",
    "# print('b')\n",
    "# accelerator.wait_for_everyone()\n",
    "# print('c')\n",
    "cfg = OmegaConf.load('conf/config.yaml')\n",
    "cfg = check_cfg_and_load_defaults(cfg)\n",
    "\n",
    "tokenized_datasets = load_dataset(cfg)\n",
    "tokenizer = load_tokenizer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (6758, 4)\n",
      "eval shape (845, 4)\n",
      "test shape (845, 4)\n"
     ]
    }
   ],
   "source": [
    "if \"train\" not in tokenized_datasets.column_names:\n",
    "    tokenized_datasets = tokenized_datasets.train_test_split(\n",
    "        test_size=cfg.training.val_split_percent / 100\n",
    "    )\n",
    "    tokenized_datasets_test_valid = tokenized_datasets[\"test\"].train_test_split(\n",
    "        test_size=0.5\n",
    "    )\n",
    "    tokenized_datasets[\"test\"] = tokenized_datasets_test_valid[\"train\"]\n",
    "    tokenized_datasets[\"validation\"] = tokenized_datasets_test_valid[\"test\"]\n",
    "    \n",
    "print(f\"train shape {tokenized_datasets['train'].shape}\")\n",
    "print(f\"eval shape {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"test shape {tokenized_datasets['test'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 6758\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'input_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_tensor(dataset):\n",
    "    for column in dataset.column_names:\n",
    "        print(f\"type of {column} is {type(dataset[column])}\")\n",
    "        # print(dataset[column][0])\n",
    "\n",
    "        # dataset[column].to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_tensor(dataset):\n",
    "    for column in dataset.column_names:\n",
    "        if column != 'text':\n",
    "            dataset[column] = torch.tensor(dataset[column]).to('cuda:0')\n",
    "        # print(f\"type of {column} is {type(dataset[column])}\")\n",
    "        # print(dataset[column][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text is <class 'list'>\n",
      "type of input_ids is <class 'list'>\n",
      "type of attention_mask is <class 'list'>\n",
      "type of labels is <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "describe_tensor(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_18277/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">970348249.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_18277/970348249.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_18277/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3644643465.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">move_to_tensor</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_18277/3644643465.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Dataset'</span> object does not support item assignment\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_18277/\u001b[0m\u001b[1;33m970348249.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_18277/970348249.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_18277/\u001b[0m\u001b[1;33m3644643465.py\u001b[0m:\u001b[94m4\u001b[0m in \u001b[92mmove_to_tensor\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_18277/3644643465.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[32m'Dataset'\u001b[0m object does not support item assignment\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "move_to_tensor(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text is <class 'list'>\n",
      "type of input_ids is <class 'list'>\n",
      "type of attention_mask is <class 'list'>\n",
      "type of labels is <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "describe_tensor(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     shuffle=True,\n",
    "#     collate_fn=default_data_collator,\n",
    "#     batch_size=cfg.training.train_batch_size,\n",
    "# )\n",
    "\n",
    "# eval_dataloader = DataLoader(\n",
    "#     eval_dataset,\n",
    "#     collate_fn=default_data_collator,\n",
    "#     batch_size=cfg.training.eval_batch_size,\n",
    "# )\n",
    "\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     collate_fn=default_data_collator,\n",
    "#     batch_size=cfg.training.eval_batch_size,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import numpy as np \n",
    "\n",
    "def attention_zeros(attn) -> Tensor:\n",
    "    attn_zeros = attn\n",
    "    idxs = np.random.randint(0, len(attn[0]-1), len(attn))\n",
    "    \n",
    "    for i, k in enumerate(zip(idxs, attn)):\n",
    "        attn_zeros[i][k[1]][k[0]] = 0\n",
    "\n",
    "    return attn_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#   model,\n",
    "#   optimizer,\n",
    "#     train_dataloader,\n",
    "#     eval_dataloader,\n",
    "#     lr_scheduler,\n",
    "# ) = accelerator.prepare(\n",
    "#         model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scheduler and math around the number of training steps.\n",
    "# overrode_max_train_steps = False\n",
    "# num_update_steps_per_epoch = math.ceil(\n",
    "#     len(train_dataloader) / cfg.training.gradient_accumulation_steps\n",
    "# )\n",
    "# if cfg.training.max_train_steps is None:\n",
    "#     cfg.training.max_train_steps = (\n",
    "#         cfg.training.num_epochs * num_update_steps_per_epoch\n",
    "#     )\n",
    "#     overrode_max_train_steps = True\n",
    "\n",
    "# # We need to recalculate our total training steps as the size of the training dataloader\n",
    "# # may have changed.\n",
    "# # num_update_steps_per_epoch = math.ceil(\n",
    "# #     len(train_dataloader) / cfg.training.gradient_accumulation_steps\n",
    "# # )\n",
    "# # if overrode_max_train_steps:\n",
    "# #     cfg.training.max_train_steps = (\n",
    "# #         cfg.training.num_epochs * num_update_steps_per_epoch\n",
    "# #     )\n",
    "# # Afterwards we recalculate our number of training epochs\n",
    "# cfg.training.num_epochs = math.ceil(\n",
    "#     cfg.training.max_train_steps / num_update_steps_per_epoch\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "from accelerate.logging import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "# import logging\n",
    "# from accelerate.logging import get_logger\n",
    "\n",
    "# if cfg.tracking.enabled is True and accelerator.is_main_process:\n",
    "#     experiment_config = vars(cfg)\n",
    "#     # TensorBoard cannot log Enums, need the raw value\n",
    "#     experiment_config[\"lr_scheduler_type\"] = cfg.training.lr_scheduler\n",
    "#     accelerator.init_trackers(\"finetune_using_clm\", experiment_config)\n",
    "\n",
    "# logger = get_logger(__name__)\n",
    "# logging.basicConfig(\n",
    "#     format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "#     datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "#     level=logging.INFO,\n",
    "# )\n",
    "    \n",
    "# logger.info(\"***** Running training *****\")\n",
    "# logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "# logger.info(f\"  Num Epochs = {cfg.training.num_epochs}\")\n",
    "# logger.info(\n",
    "#     f\"  Gradient Accumulation steps = {cfg.training.gradient_accumulation_steps}\"\n",
    "# )\n",
    "# logger.info(f\"  Total optimization steps = {cfg.training.max_train_steps}\")\n",
    "\n",
    "# # Only show the progress bar once on each machine.\n",
    "# progress_bar = tqdm(\n",
    "#     range(cfg.training.max_train_steps),\n",
    "#     disable=not accelerator.is_local_main_process,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0, 1, 2, 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop():\n",
    "    \n",
    "    cfg = OmegaConf.load('conf/config.yaml')\n",
    "    cfg = check_cfg_and_load_defaults(cfg)\n",
    "\n",
    "    tokenized_datasets = load_dataset(cfg)\n",
    "    tokenizer = load_tokenizer(cfg)\n",
    "\n",
    "    print('a')\n",
    "    accelerator = create_accelerator(cfg)\n",
    "    print('b')\n",
    "    accelerator.wait_for_everyone()\n",
    "    print('c')\n",
    "\n",
    "    model = load_model(cfg, tokenizer)\n",
    "    optimizer = create_optimizer(cfg, model)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=cfg.training.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=cfg.training.lr_warmup_steps,\n",
    "        num_training_steps=cfg.training.max_train_steps,\n",
    "    )\n",
    "\n",
    "\n",
    "    if \"train\" not in tokenized_datasets.column_names:\n",
    "        tokenized_datasets = tokenized_datasets.train_test_split(\n",
    "            test_size=cfg.training.val_split_percent / 100\n",
    "        )\n",
    "        tokenized_datasets_test_valid = tokenized_datasets[\"test\"].train_test_split(\n",
    "            test_size=0.5\n",
    "        )\n",
    "        tokenized_datasets[\"test\"] = tokenized_datasets_test_valid[\"train\"]\n",
    "        tokenized_datasets[\"validation\"] = tokenized_datasets_test_valid[\"test\"]\n",
    "        \n",
    "    print(f\"train shape {tokenized_datasets['train'].shape}\")\n",
    "    print(f\"eval shape {tokenized_datasets['validation'].shape}\")\n",
    "    print(f\"test shape {tokenized_datasets['test'].shape}\")\n",
    "    \n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    eval_dataset = tokenized_datasets[\"validation\"]\n",
    "    test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=cfg.training.train_batch_size,\n",
    "    )\n",
    "\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=cfg.training.eval_batch_size,\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        collate_fn=default_data_collator,\n",
    "        batch_size=cfg.training.eval_batch_size,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    overrode_max_train_steps = False\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / cfg.training.gradient_accumulation_steps\n",
    "    )\n",
    "    if cfg.training.max_train_steps is None:\n",
    "        cfg.training.max_train_steps = (\n",
    "            cfg.training.num_epochs * num_update_steps_per_epoch\n",
    "        )\n",
    "        overrode_max_train_steps = True\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader\n",
    "    # may have changed.\n",
    "    # num_update_steps_per_epoch = math.ceil(\n",
    "    #     len(train_dataloader) / cfg.training.gradient_accumulation_steps\n",
    "    # )\n",
    "    # if overrode_max_train_steps:\n",
    "    #     cfg.training.max_train_steps = (\n",
    "    #         cfg.training.num_epochs * num_update_steps_per_epoch\n",
    "    #     )\n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    cfg.training.num_epochs = math.ceil(\n",
    "        cfg.training.max_train_steps / num_update_steps_per_epoch\n",
    "    )\n",
    "\n",
    "\n",
    "    if cfg.tracking.enabled is True and accelerator.is_main_process:\n",
    "        experiment_config = vars(cfg)\n",
    "        # TensorBoard cannot log Enums, need the raw value\n",
    "        experiment_config[\"lr_scheduler_type\"] = cfg.training.lr_scheduler\n",
    "        accelerator.init_trackers(\"finetune_using_clm\", experiment_config)\n",
    "\n",
    "    logger = get_logger(__name__)\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "        \n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num Epochs = {cfg.training.num_epochs}\")\n",
    "    logger.info(\n",
    "        f\"  Gradient Accumulation steps = {cfg.training.gradient_accumulation_steps}\"\n",
    "    )\n",
    "    logger.info(f\"  Total optimization steps = {cfg.training.max_train_steps}\")\n",
    "\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(\n",
    "        range(cfg.training.max_train_steps),\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    completed_steps = 0\n",
    "    starting_epoch = 0\n",
    "\n",
    "    # Add resume training functionality\n",
    "\n",
    "    epoch_durations = []\n",
    "    train_checkpoint_durations = []\n",
    "\n",
    "    with wandb.init(project=cfg.project_name, config=cfg, name=cfg.model.name):\n",
    "        for epoch in range(starting_epoch, cfg.training.num_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            model.train()\n",
    "            # if cfg.tracking.enabled is True:\n",
    "            total_loss = 0\n",
    "            train_losses = []\n",
    "            train_checkpoint_start_time = time.time()\n",
    "\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                # We need to skip steps until we reach the resumed step\n",
    "                # if (\n",
    "                #     cfg.training.checkpoint.resume_from_checkpoint\n",
    "                #     and epoch == starting_epoch\n",
    "                # ):\n",
    "                #     if resume_step is not None and step < resume_step:\n",
    "                #         completed_steps += 1\n",
    "                #         continue\n",
    "                \n",
    "                # Set random token to 0 attention\n",
    "                tmp_attn = batch['attention_mask']\n",
    "                batch['attention_mask'] = attention_zeros(tmp_attn)\n",
    "\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                train_losses.append(\n",
    "                    accelerator.gather(loss.repeat(cfg.training.train_batch_size))\n",
    "                )\n",
    "            # We keep track of the loss at each epoch\n",
    "                if cfg.tracking.enabled is True:\n",
    "                    total_loss += loss.detach().float()\n",
    "                loss = loss / cfg.training.gradient_accumulation_steps\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                if (\n",
    "                        step % cfg.training.gradient_accumulation_steps == 0\n",
    "                        or step == len(train_dataloader) - 1\n",
    "                    ):\n",
    "                        optimizer.step()\n",
    "                        lr_scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        progress_bar.update(1)\n",
    "                        completed_steps += 1\n",
    "\n",
    "\n",
    "                if step % cfg.training.eval_every == 0:\n",
    "                    \n",
    "\n",
    "                    train_losses_tensor = torch.cat(train_losses)\n",
    "                    train_loss = torch.mean(train_losses_tensor)\n",
    "                    model.eval()\n",
    "                    eval_losses = []\n",
    "                    for _eval_step, eval_batch in enumerate(eval_dataloader):\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(**eval_batch)\n",
    "\n",
    "                        loss = outputs.loss\n",
    "                        eval_losses.append(\n",
    "                            accelerator.gather(loss.repeat(cfg.training.eval_batch_size))\n",
    "                        )\n",
    "\n",
    "                    losses = torch.cat(eval_losses)\n",
    "                    losses = losses[: len(eval_dataset)]\n",
    "                    try:\n",
    "                        eval_loss = torch.mean(losses)\n",
    "                        perplexity = math.exp(eval_loss)\n",
    "                    except OverflowError:\n",
    "                        perplexity = float(\"inf\")\n",
    "\n",
    "                    train_checkpoint_duration = time.time() - train_checkpoint_start_time\n",
    "                    train_checkpoint_durations.append(train_checkpoint_duration)\n",
    "                    \n",
    "                    wandb.log({\"train_loss\": train_loss, \"epoch\": epoch, 'eval_loss': eval_loss, 'eval_perplexity': perplexity, 'train_checkpoint_duration': train_checkpoint_duration}, step=step)\n",
    "                    logger.info(\n",
    "                        f\"epoch {epoch}: eval_perplexity: {perplexity} train_loss: {train_loss} eval_loss: {eval_loss} 'train_checkpoint_duration': {train_checkpoint_duration} step: {step}\"\n",
    "                    )\n",
    "\n",
    "                    train_checkpoint_start_time = time.time()\n",
    "\n",
    "                    # epoch_dir = f\"epoch_{epoch}_most_recent\"\n",
    "                    # if cfg.output_dir is not None:\n",
    "                    #     output_dir = os.path.join(cfg.output_dir, epoch_dir)\n",
    "                    # unwrapped_model = accelerator.unwrap_model(model)\n",
    "                    # unwrapped_model.save_pretrained(\n",
    "                    #     output_dir,\n",
    "                    #     is_main_process=accelerator.is_main_process,\n",
    "                    #     save_function=accelerator.save,\n",
    "                    # )\n",
    "                    # if accelerator.is_main_process:\n",
    "                    #     tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            train_loss = total_loss.item() / len(train_dataloader)\n",
    "\n",
    "            # Below was causing WandB communication errors\n",
    "            # if cfg.tracking.enabled is True:\n",
    "            #     accelerator.log(\n",
    "            #         {\n",
    "            #             \"perplexity\": perplexity,\n",
    "            #             \"eval_loss\": eval_loss,\n",
    "            #             \"train_loss\": train_loss,\n",
    "            #             \"epoch\": epoch,\n",
    "            #             \"step\": completed_steps,\n",
    "            #         },\n",
    "            #         step=completed_steps,\n",
    "            #     )\n",
    "\n",
    "\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            epoch_durations.append(epoch_duration)\n",
    "\n",
    "            wandb.log({\"train_loss\": train_loss, \"epoch\": epoch, 'eval_loss': eval_loss, 'eval_perplexity': perplexity, 'epoch_duration': epoch_duration}, step=step)\n",
    "            logger.info(f\"done epoch {epoch}\")\n",
    "\n",
    "\n",
    "        avg_epoch_runtime = sum(epoch_durations) / len(epoch_durations)\n",
    "        avg_train_checkpoint_runtime = sum(train_checkpoint_durations) / len(train_checkpoint_durations)\n",
    "\n",
    "        wandb.log({\"avg epoch runtime (seconds)\": avg_epoch_runtime})\n",
    "        wandb.log({\"avg train checkpoint runtime (seconds)\": avg_train_checkpoint_runtime})\n",
    "        wandb.finish()\n",
    "\n",
    "    if cfg.output_dir is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            cfg.output_dir,\n",
    "            is_main_process=accelerator.is_main_process,\n",
    "            save_function=accelerator.save,\n",
    "        )\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(cfg.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "loaded data from /mnt/share/ipfs-data/data/tokenized_data_0.\n",
      "/mnt/share/ipfs-data/data/tokenized_data_1 doesn't exist.\n",
      "loaded data from /mnt/share/ipfs-data/data/tokenized_data_0.\n",
      "/mnt/share/ipfs-data/data/tokenized_data_1 doesn't exist.\n",
      "loaded data from /mnt/share/ipfs-data/data/tokenized_data_0.\n",
      "/mnt/share/ipfs-data/data/tokenized_data_1 doesn't exist.\n",
      "loaded data from /mnt/share/ipfs-data/data/tokenized_data_0.\n",
      "/mnt/share/ipfs-data/data/tokenized_data_1 doesn't exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_24402/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">631038997.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_24402/631038997.py'</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">launchers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">36</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">notebook_launcher</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">133 │   │   │   │   </span>launcher = PrepareForLaunch(function, distributed_type=<span style=\"color: #808000; text-decoration-color: #808000\">\"MULTI_GPU\"</span>)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">134 │   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">135 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Launching training on {</span>num_processes<span style=\"color: #808000; text-decoration-color: #808000\">} GPUs.\"</span>)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>136 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>start_processes(launcher, args=args, nprocs=num_processes, start_method=   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">138 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 │   │   │   # No need for a distributed launch otherwise as it's either CPU, GPU or MPS.</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/torch/multiprocessing/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">spa</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">wn.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">198</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">start_processes</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> context                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # Loop on join until it returns True or raises an exception.</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>198 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> context.join():                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">pass</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/torch/multiprocessing/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">spa</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">wn.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">109</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">join</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">106 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">108 │   │   # Wait for any process to fail or all of them to succeed.</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>109 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>ready = multiprocessing.connection.wait(                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.sentinels.keys(),                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 │   │   │   </span>timeout=timeout,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/lib/python3.9/multiprocessing/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">connection.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">931</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wait</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">928 │   │   │   │   </span>deadline = time.monotonic() + timeout                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">929 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">930 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>931 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>ready = selector.select(timeout)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">932 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> ready:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">933 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> [key.fileobj <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> (key, events) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> ready]                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">934 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/lib/python3.9/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">selectors.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">416</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">select</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">413 │   │   │   </span>timeout = math.ceil(timeout * <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1e3</span>)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">414 │   │   </span>ready = []                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">415 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>416 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>fd_event_list = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._selector.poll(timeout)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">417 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">InterruptedError</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">418 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> ready                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">419 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> fd, event <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> fd_event_list:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_24402/\u001b[0m\u001b[1;33m631038997.py\u001b[0m:\u001b[94m3\u001b[0m in \u001b[92m<module>\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_24402/631038997.py'\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/accelerate/\u001b[0m\u001b[1;33mlaunchers.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m36\u001b[0m in \u001b[92mnotebook_launcher\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m133 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlauncher = PrepareForLaunch(function, distributed_type=\u001b[33m\"\u001b[0m\u001b[33mMULTI_GPU\u001b[0m\u001b[33m\"\u001b[0m)        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m134 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLaunching training on \u001b[0m\u001b[33m{\u001b[0mnum_processes\u001b[33m}\u001b[0m\u001b[33m GPUs.\u001b[0m\u001b[33m\"\u001b[0m)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m136 \u001b[2m│   │   │   │   \u001b[0mstart_processes(launcher, args=args, nprocs=num_processes, start_method=   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# No need for a distributed launch otherwise as it's either CPU, GPU or MPS.\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/torch/multiprocessing/\u001b[0m\u001b[1;33mspa\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mwn.py\u001b[0m:\u001b[94m198\u001b[0m in \u001b[92mstart_processes\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m context                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Loop on join until it returns True or raises an exception.\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m198 \u001b[2m│   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[95mnot\u001b[0m context.join():                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mpass\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/torch/multiprocessing/\u001b[0m\u001b[1;33mspa\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mwn.py\u001b[0m:\u001b[94m109\u001b[0m in \u001b[92mjoin\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[94mTrue\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Wait for any process to fail or all of them to succeed.\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m109 \u001b[2m│   │   \u001b[0mready = multiprocessing.connection.wait(                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.sentinels.keys(),                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   \u001b[0mtimeout=timeout,                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3.9/multiprocessing/\u001b[0m\u001b[1;33mconnection.py\u001b[0m:\u001b[94m931\u001b[0m in \u001b[92mwait\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m928 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdeadline = time.monotonic() + timeout                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m929 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m930 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m931 \u001b[2m│   │   │   │   \u001b[0mready = selector.select(timeout)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m932 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m ready:                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m933 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m [key.fileobj \u001b[94mfor\u001b[0m (key, events) \u001b[95min\u001b[0m ready]                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m934 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3.9/\u001b[0m\u001b[1;33mselectors.py\u001b[0m:\u001b[94m416\u001b[0m in \u001b[92mselect\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m413 \u001b[0m\u001b[2m│   │   │   \u001b[0mtimeout = math.ceil(timeout * \u001b[94m1e3\u001b[0m)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m414 \u001b[0m\u001b[2m│   │   \u001b[0mready = []                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m415 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m416 \u001b[2m│   │   │   \u001b[0mfd_event_list = \u001b[96mself\u001b[0m._selector.poll(timeout)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m417 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mInterruptedError\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m418 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m ready                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m419 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m fd, event \u001b[95min\u001b[0m fd_event_list:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_loop, num_processes=4, use_port=29504)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "loaded data from /mnt/share/ipfs-data/data/tokenized_data_0.loaded data from /mnt/share/ipfs-data/data/tokenized_data_0.loaded data from /mnt/share/ipfs-data/data/tokenized_data_0.loaded data from /mnt/share/ipfs-data/data/tokenized_data_0.\n",
      "\n",
      "\n",
      "\n",
      "/mnt/share/ipfs-data/data/tokenized_data_1 doesn't exist./mnt/share/ipfs-data/data/tokenized_data_1 doesn't exist./mnt/share/ipfs-data/data/tokenized_data_1 doesn't exist./mnt/share/ipfs-data/data/tokenized_data_1 doesn't exist.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n",
      "  warnings.warn(\n",
      "Using pad_token, but it is not set yet.\n",
      "/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/accelerate/accelerator.py:249: FutureWarning: `logging_dir` is deprecated and will be removed in version 0.18.0 of 🤗 Accelerate. Use `project_dir` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_22371/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">3134166966.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_22371/3134166966.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">launchers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">36</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">notebook_launcher</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">133 │   │   │   │   </span>launcher = PrepareForLaunch(function, distributed_type=<span style=\"color: #808000; text-decoration-color: #808000\">\"MULTI_GPU\"</span>)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">134 │   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">135 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f\"Launching training on {</span>num_processes<span style=\"color: #808000; text-decoration-color: #808000\">} GPUs.\"</span>)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>136 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>start_processes(launcher, args=args, nprocs=num_processes, start_method=   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">137 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">138 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 │   │   │   # No need for a distributed launch otherwise as it's either CPU, GPU or MPS.</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/torch/multiprocessing/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">spa</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">wn.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">198</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">start_processes</span>                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> context                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">197 │   # Loop on join until it returns True or raises an exception.</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>198 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> context.join():                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">pass</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/torch/multiprocessing/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">spa</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">wn.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">109</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">join</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">106 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">107 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">108 │   │   # Wait for any process to fail or all of them to succeed.</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>109 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>ready = multiprocessing.connection.wait(                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.sentinels.keys(),                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 │   │   │   </span>timeout=timeout,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/lib/python3.9/multiprocessing/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">connection.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">931</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wait</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">928 │   │   │   │   </span>deadline = time.monotonic() + timeout                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">929 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">930 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>:                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>931 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>ready = selector.select(timeout)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">932 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> ready:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">933 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> [key.fileobj <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> (key, events) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> ready]                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">934 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/lib/python3.9/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">selectors.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">416</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">select</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">413 │   │   │   </span>timeout = math.ceil(timeout * <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1e3</span>)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">414 │   │   </span>ready = []                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">415 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>416 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>fd_event_list = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._selector.poll(timeout)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">417 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">InterruptedError</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">418 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> ready                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">419 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> fd, event <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> fd_event_list:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_22371/\u001b[0m\u001b[1;33m3134166966.py\u001b[0m:\u001b[94m3\u001b[0m in \u001b[92m<module>\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_22371/3134166966.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/accelerate/\u001b[0m\u001b[1;33mlaunchers.py\u001b[0m:\u001b[94m1\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[94m36\u001b[0m in \u001b[92mnotebook_launcher\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m133 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlauncher = PrepareForLaunch(function, distributed_type=\u001b[33m\"\u001b[0m\u001b[33mMULTI_GPU\u001b[0m\u001b[33m\"\u001b[0m)        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m134 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m135 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLaunching training on \u001b[0m\u001b[33m{\u001b[0mnum_processes\u001b[33m}\u001b[0m\u001b[33m GPUs.\u001b[0m\u001b[33m\"\u001b[0m)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m136 \u001b[2m│   │   │   │   \u001b[0mstart_processes(launcher, args=args, nprocs=num_processes, start_method=   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# No need for a distributed launch otherwise as it's either CPU, GPU or MPS.\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/torch/multiprocessing/\u001b[0m\u001b[1;33mspa\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mwn.py\u001b[0m:\u001b[94m198\u001b[0m in \u001b[92mstart_processes\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m context                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Loop on join until it returns True or raises an exception.\u001b[0m                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m198 \u001b[2m│   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[95mnot\u001b[0m context.join():                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mpass\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/paperspace/Desktop/clm-training/venv/lib/python3.9/site-packages/torch/multiprocessing/\u001b[0m\u001b[1;33mspa\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mwn.py\u001b[0m:\u001b[94m109\u001b[0m in \u001b[92mjoin\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[94mTrue\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Wait for any process to fail or all of them to succeed.\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m109 \u001b[2m│   │   \u001b[0mready = multiprocessing.connection.wait(                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.sentinels.keys(),                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   \u001b[0mtimeout=timeout,                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3.9/multiprocessing/\u001b[0m\u001b[1;33mconnection.py\u001b[0m:\u001b[94m931\u001b[0m in \u001b[92mwait\u001b[0m                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m928 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdeadline = time.monotonic() + timeout                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m929 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m930 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[94mTrue\u001b[0m:                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m931 \u001b[2m│   │   │   │   \u001b[0mready = selector.select(timeout)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m932 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m ready:                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m933 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m [key.fileobj \u001b[94mfor\u001b[0m (key, events) \u001b[95min\u001b[0m ready]                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m934 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/lib/python3.9/\u001b[0m\u001b[1;33mselectors.py\u001b[0m:\u001b[94m416\u001b[0m in \u001b[92mselect\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m413 \u001b[0m\u001b[2m│   │   │   \u001b[0mtimeout = math.ceil(timeout * \u001b[94m1e3\u001b[0m)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m414 \u001b[0m\u001b[2m│   │   \u001b[0mready = []                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m415 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m416 \u001b[2m│   │   │   \u001b[0mfd_event_list = \u001b[96mself\u001b[0m._selector.poll(timeout)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m417 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mInterruptedError\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m418 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m ready                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m419 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m fd, event \u001b[95min\u001b[0m fd_event_list:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_loop, num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 5562,   717,  2239,  ..., 26350,   329,  1811],\n",
      "        [ 2435,   290,   345,  ...,   326,   339,   550],\n",
      "        [ 1662,  3051,   287,  ...,    11,   810,   262],\n",
      "        ...,\n",
      "        [ 1462,   423,   640,  ..., 10846,  3332,   287],\n",
      "        [13893,   356,   550,  ...,   503,   612,   379],\n",
      "        [  505,  1517,   878,  ...,   503,   465,  4324]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[ 5562,   717,  2239,  ..., 26350,   329,  1811],\n",
      "        [ 2435,   290,   345,  ...,   326,   339,   550],\n",
      "        [ 1662,  3051,   287,  ...,    11,   810,   262],\n",
      "        ...,\n",
      "        [ 1462,   423,   640,  ..., 10846,  3332,   287],\n",
      "        [13893,   356,   550,  ...,   503,   612,   379],\n",
      "        [  505,  1517,   878,  ...,   503,   465,  4324]])}\n",
      "length batch 256\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    # We need to skip steps until we reach the resumed step\n",
    "    # if (\n",
    "    #     cfg.training.checkpoint.resume_from_checkpoint\n",
    "    #     and epoch == starting_epoch\n",
    "    # ):\n",
    "    #     if resume_step is not None and step < resume_step:\n",
    "    #         completed_steps += 1\n",
    "    #         continue\n",
    "\n",
    "    print(batch)\n",
    "    print(f\"length batch {len(batch['attention_mask'][0])}\")\n",
    "    attn = batch['input_ids']\n",
    "# print(**batch)\n",
    "\n",
    "    break\n",
    "    # outputs = model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22971"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(train_dataloader)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "hi = np.random.randint(0, 255, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_z = attention_zeros(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(attn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(attn_z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_zeros = attn\n",
    "\n",
    "for i, k in enumerate(zip(hi, attn)):\n",
    "    # \n",
    "    # print(k)\n",
    "    attn_zeros[i][k[1]][k[0]] = 0\n",
    "    # print(k)\n",
    "    # print(k[0])\n",
    "    # print(k[1])\n",
    "    # print(k[hi])\n",
    "    # break\n",
    "    # print(k)\n",
    "\n",
    "# new_attn = atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_zeros[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bittensor",
   "language": "python",
   "name": "bittensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
