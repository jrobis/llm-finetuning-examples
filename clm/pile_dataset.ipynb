{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlamaTokenizer\n",
    ")\n",
    "\n",
    "# import bittensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': '/mnt/share/the_pile/val.jsonl.zst'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "base_url = '/mnt/share/the_pile/'\n",
    "data_files = {\n",
    "    # \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(1)],\n",
    "    \"validation\": base_url + \"val.jsonl.zst\",\n",
    "    # \"test\": base_url + \"test.jsonl.zst\",\n",
    "}\n",
    "\n",
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/paperspace/.cache/huggingface/datasets/json/default-9a2dd9e55d64cca5/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 7913.78it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:06<00:00,  6.05s/it]\n",
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/paperspace/.cache/huggingface/datasets/json/default-9a2dd9e55d64cca5/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 195.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "pile_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "# pile_dataset_streamed = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "# next(iter(pile_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\", config=\"/mnt/share/llama-7b-hf/tokenizer_config.json\", cache_dir='/mnt/share/.cache/huggingface/hub')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                            \r"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load('conf/config.yaml')\n",
    "\n",
    "column_names = pile_dataset.column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[\"train\"][0]\n",
    "if cfg.dataset.concatenate_raw is True:\n",
    "    pad = False\n",
    "else:\n",
    "    pad = \"max_length\"\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= cfg.dataset.block_size:\n",
    "        total_length = (\n",
    "            total_length // cfg.dataset.block_size\n",
    "        ) * cfg.dataset.block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [\n",
    "            t[i : i + cfg.dataset.block_size]\n",
    "            for i in range(0, total_length, cfg.dataset.block_size)\n",
    "        ]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    result = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=pad,\n",
    "        truncation=True,\n",
    "        max_length=cfg.dataset.block_size,\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = pile_dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        num_proc=cfg.tokenizer.preprocessing_num_workers,\n",
    "        load_from_cache_file=not cfg.dataset.overwrite_cache,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = list((tokenized_datasets['train'].column_names))\n",
    "cols_to_remove.remove('input_ids')\n",
    "cols_to_remove.remove('attention_mask')\n",
    "cols_to_remove.remove('labels')\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(cols_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 214670\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    }
   ],
   "source": [
    "tokenized_datasets.save_to_disk('/mnt/share/the_pile/tokenized_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "loaded_dataset = load_from_disk(\"/mnt/share/the_pile/tokenized_val\")['train'].with_format(None).to_iterable_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loaded_dataset)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
