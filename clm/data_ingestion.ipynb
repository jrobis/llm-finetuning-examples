{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs below on the venv in terminal\n",
    "# pip install tqdm\n",
    "# pip install bittensor\n",
    "# pip install datasets\n",
    "# pip install omegaconf\n",
    "# pip install hydra-core --upgrade\n",
    "# pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING]: failed to patch stdout/stderr for fork-safety: 'OutStream' object\n",
      "has no attribute 'buffer'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "import bittensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cfg_and_load_defaults(cfg: DictConfig) -> DictConfig:\n",
    "\n",
    "    subtensor = bittensor.subtensor(network=cfg.bittensor.network)\n",
    "    if cfg.dataset.block_size is None:\n",
    "        cfg.dataset.block_size = subtensor.validator_sequence_length\n",
    "    if cfg.training.train_batch_size is None:\n",
    "        cfg.training.train_batch_size = subtensor.validator_batch_size\n",
    "    if cfg.training.eval_batch_size is None:\n",
    "        cfg.training.eval_batch_size = subtensor.validator_batch_size\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_datasets(cfg: DictConfig) -> DatasetDict:\n",
    "\n",
    "    if cfg.dataset.name == \"bittensor\":\n",
    "\n",
    "        dataset = bittensor.dataset(\n",
    "            no_tokenizer=True,\n",
    "            batch_size=cfg.training.train_batch_size,\n",
    "            block_size=cfg.dataset.block_size,\n",
    "        )\n",
    "        dataloader = dataset.dataloader(cfg.dataset.num_batches)\n",
    "        bittensor_dataset = {\"text\": []}\n",
    "        for batch in tqdm(dataloader, desc=\"Loading data from bittensor IPFS\"):\n",
    "            bittensor_dataset[\"text\"].extend(batch)\n",
    "        raw_datasets = Dataset.from_dict(bittensor_dataset)\n",
    "\n",
    "        dataset.close()  # Avoid leaving threadqueue running.\n",
    "        return raw_datasets\n",
    "\n",
    "    if os.path.exists(cfg.dataset.name):\n",
    "        data_files = {\"text\": cfg.dataset.name}\n",
    "        dataset_args = {}\n",
    "\n",
    "        extension = os.path.splitext(cfg.dataset.name)[-1].lstrip(\".\")\n",
    "\n",
    "        if extension == \"txt\":\n",
    "            extension = \"text\"\n",
    "            dataset_args[\"keep_linebreaks\"] = cfg.dataset.keep_linebreaks\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)\n",
    "        raw_datasets = raw_datasets[\"text\"]\n",
    "    else:\n",
    "        raw_datasets = load_dataset(cfg.dataset.name, cfg.dataset.config_name)\n",
    "\n",
    "    return raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(cfg: DictConfig):\n",
    "    if cfg.tokenizer.name is not None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            cfg.tokenizer.name, use_fast=cfg.tokenizer.use_fast\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            cfg.model.name, use_fast=cfg.tokenizer.use_fast\n",
    "        )\n",
    "    \n",
    "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_accelerator(cfg: DictConfig) -> Accelerator:\n",
    "\n",
    "    accelerator = (\n",
    "        Accelerator(log_with=cfg.tracking.report_to, logging_dir=cfg.output_dir)\n",
    "        if cfg.tracking.enabled\n",
    "        else Accelerator()\n",
    "    )\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    return accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(cfg: DictConfig, tokenizer, raw_datasets):\n",
    "    # First we tokenize all the texts.\n",
    "    column_names = raw_datasets.column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[\"train\"][0]\n",
    "    if cfg.dataset.concatenate_raw is True:\n",
    "        pad = False\n",
    "    else:\n",
    "        pad = \"max_length\"\n",
    "    \n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        if total_length >= cfg.dataset.block_size:\n",
    "            total_length = (\n",
    "                total_length // cfg.dataset.block_size\n",
    "            ) * cfg.dataset.block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [\n",
    "                t[i : i + cfg.dataset.block_size]\n",
    "                for i in range(0, total_length, cfg.dataset.block_size)\n",
    "            ]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        result = tokenizer(\n",
    "            examples[text_column_name],\n",
    "            padding=pad,\n",
    "            truncation=True,\n",
    "            max_length=cfg.dataset.block_size,\n",
    "        )\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_fn,\n",
    "            batched=True,\n",
    "            num_proc=cfg.tokenizer.preprocessing_num_workers,\n",
    "            load_from_cache_file=not cfg.dataset.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "\n",
    "        if cfg.dataset.concatenate_raw is True:\n",
    "            tokenized_datasets = tokenized_datasets.map(\n",
    "                group_texts,\n",
    "                batched=True,\n",
    "                num_proc=cfg.tokenizer.preprocessing_num_workers,\n",
    "                load_from_cache_file=not cfg.dataset.overwrite_cache,\n",
    "                desc=f\"Grouping texts in chunks of {cfg.dataset.block_size}\",\n",
    "            )\n",
    "\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def write_tokenized_datasets(cfg: DictConfig, accelerator, tokenizer, loops=50) -> None:\n",
    "    i = 0\n",
    "    new_dataset = True\n",
    "    for k in range(loops):\n",
    "        raw_datasets = load_raw_datasets(cfg)\n",
    "        tokenized_dataset_batch = preprocess(cfg, tokenizer, raw_datasets)\n",
    "\n",
    "        if new_dataset:\n",
    "            tokenized_dataset = tokenized_dataset_batch\n",
    "            new_dataset = False\n",
    "\n",
    "        else:\n",
    "            tokenized_dataset = concatenate_datasets([tokenized_dataset, tokenized_dataset_batch])\n",
    "\n",
    "        \n",
    "        if tokenized_dataset.shape[0] > 100000:\n",
    "            file_name = cfg.dataset.file_name + \"_\" + str(i)\n",
    "            output_file = os.path.join(cfg.dataset.data_dir, file_name)\n",
    "\n",
    "            tokenized_dataset.save_to_disk(output_file)\n",
    "            i += 1\n",
    "            new_dataset=True\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('conf/config.yaml')\n",
    "cfg = check_cfg_and_load_defaults(cfg)\n",
    "\n",
    "accelerator = create_accelerator(cfg)\n",
    "accelerator.wait_for_everyone()\n",
    "tokenizer = load_tokenizer(cfg)\n",
    "\n",
    "write_tokenized_datasets(cfg, accelerator, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "bittensor-ipfs"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
