{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf.dictconfig import DictConfig\n",
    "\n",
    "import datasets\n",
    "import transformers\n",
    "from accelerate import Accelerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"/mnt/share/the_pile/\"\n",
    "data_files = {\n",
    "    \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(1)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['/mnt/share/the_pile/train/00.jsonl.zst']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized00.josnl\n"
     ]
    }
   ],
   "source": [
    "for subset in data_files.keys():\n",
    "    for _file in data_files[subset]:\n",
    "       write_tokenized_file(_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokenized_file(_file):\n",
    "    tokenized_path = '/mnt/share/tokenized_pile'\n",
    "    # tokenized_filename = _file...\n",
    "    tokenized_filename = \"tokenized_\"+_file[_file.find('.')-2:_file.find('.')]+ \".josnl\"\n",
    "    print(tokenized_filename)\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('conf/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(cfg: DictConfig):\n",
    "    if cfg.tokenizer.name is not None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            cfg.tokenizer.name, use_fast=cfg.tokenizer.use_fast\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            cfg.model.name, use_fast=cfg.tokenizer.use_fast\n",
    "        )\n",
    "    \n",
    "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 619/619 [00:00<00:00, 89.7kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 60.3MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 34.6MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.37M/1.37M [00:00<00:00, 53.2MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 4.04k/4.04k [00:00<00:00, 1.66MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 357/357 [00:00<00:00, 157kB/s]\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2TokenizerFast(name_or_path='EleutherAI/gpt-j-6b', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 14.9MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 318B/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 141/141 [00:00<00:00, 68.5kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\", config=\"/mnt/share/llama-7b-hf/tokenizer_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "def load_tokenizer():\n",
    "\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\", config=\"/mnt/share/llama-7b-hf/tokenizer_config.json\")\n",
    "    # cache_dir='/mnt/share-4tb/.cache/huggingface/hub'\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(cfg: DictConfig):\n",
    "    if cfg.tokenizer.name is not None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            cfg.tokenizer.name, use_fast=cfg.tokenizer.use_fast\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            # cfg.model.name, use_fast=cfg.tokenizer.use_fast\n",
    "            cfg.model.name\n",
    "        )\n",
    "    \n",
    "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[39m=\u001b[39m load_tokenizer()  \n",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m, in \u001b[0;36mload_tokenizer\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tokenizer\u001b[39m():\n\u001b[0;32m----> 5\u001b[0m     tokenizer \u001b[39m=\u001b[39m LlamaTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mdecapoda-research/llama-7b-hf\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/mnt/share/llama-7b-hf/tokenizer_config.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m     \u001b[39m# cache_dir='/mnt/share-4tb/.cache/huggingface/hub'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[39m# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(cfg: DictConfig):\n",
    "    if cfg.tokenizer.name is not None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            cfg.tokenizer.name, use_fast=cfg.tokenizer.use_fast\n",
    "        )\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            # cfg.model.name, use_fast=cfg.tokenizer.use_fast\n",
    "            cfg.model.name\n",
    "        )\n",
    "    \n",
    "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def load_datasets(cfg: DictConfig, logger):\n",
    "\n",
    "    # base_url = \"https://the-eye.eu/public/AI/pile/\"\n",
    "    base_url = '/mnt/share-4tb/the_pile/'\n",
    "    data_files = {\n",
    "        \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(1)],\n",
    "        \"eval\": base_url + \"val.jsonl.zst\",\n",
    "        # \"test\": base_url + \"test.jsonl.zst\",\n",
    "    }\n",
    "\n",
    "    # data_files\n",
    "    # from datasets import load_dataset\n",
    "\n",
    "    pile_dataset_streamed = load_dataset(\"json\", data_files=data_files, streaming=True)\n",
    "\n",
    "    def cast_to_tensor(_dict):\n",
    "        _tmp = {k: torch.tensor(v, dtype=torch.long) for k, v in _dict.items() if k in ['input_ids', 'attention_mask', 'labels']}\n",
    "        return _tmp\n",
    "\n",
    "    # first option\n",
    "    def add_labels(example):\n",
    "        return {\"labels\": example[\"input_ids\"]}\n",
    "    \n",
    "    # def combine(example):\n",
    "    #     _dict = {k: torch.tensor(v, dtype=torch.long) for k, v in example.items() if k in ['input_ids', 'attention_mask']}\n",
    "    #     _dict[\"labels\"] = _dict[\"input_ids\"]\n",
    "    #     return _dict\n",
    "\n",
    "    tokenizer = load_tokenizer(cfg)    \n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    # tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    # if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "    #     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    train_tokenized_dataset = pile_dataset_streamed['train'].map(lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True, max_length=256), batched=True, batch_size=3_200)\n",
    "    train_tokenized_dataset = train_tokenized_dataset.remove_columns(['text', 'meta'])\n",
    "    # train_tokenized_dataset = train_tokenized_dataset.map(lambda x: combine(x))\n",
    "    train_tokenized_dataset = train_tokenized_dataset.map(lambda x: add_labels(x))\n",
    "    # train_tokenized_dataset = train_tokenized_dataset.map(lambda x: cast_to_tensor(x))\n",
    "\n",
    "    eval_tokenized_dataset = pile_dataset_streamed['eval'].map(lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True, max_length=256), batched=True, batch_size=3_200)\n",
    "    eval_tokenized_dataset = eval_tokenized_dataset.remove_columns(['text', 'meta'])\n",
    "    eval_tokenized_dataset = eval_tokenized_dataset.map(lambda x: add_labels(x))\n",
    "    eval_tokenized_dataset = eval_tokenized_dataset.map(lambda x: cast_to_tensor(x))\n",
    "\n",
    "\n",
    "    train_tokenized_dataset = train_tokenized_dataset.shuffle(seed=cfg.training.seed, buffer_size=6_400)\n",
    "    eval_tokenized_dataset = eval_tokenized_dataset.shuffle(seed=cfg.training.seed, buffer_size=6_400)\n",
    "    # print(next(iter(tokenized_dataset_tensor)).keys())\n",
    "    # status = True\n",
    "    # i = 0\n",
    "    # while status:\n",
    "    #     file_name = cfg.dataset.file_name + \"_\" + str(i)\n",
    "    #     data_file = os.path.join(cfg.dataset.data_dir, file_name)\n",
    "\n",
    "    #     try:\n",
    "    #         tokenized_dataset_batch = load_from_disk(data_file)\n",
    "    #         tokenized_dataset_batch.set_format(type='pt')\n",
    "    #         logger.info(f\"loaded data from {data_file}.\")\n",
    "    #     except:\n",
    "    #         status = False\n",
    "    #         logger.info(f\"{data_file} doesn't exist.\")\n",
    "\n",
    "    #     if i==0:\n",
    "    #         tokenized_dataset = tokenized_dataset_batch\n",
    "    #     else:\n",
    "    #         tokenized_dataset = concatenate_datasets([tokenized_dataset, tokenized_dataset_batch])\n",
    "        \n",
    "    #     i += 1\n",
    "\n",
    "    return train_tokenized_dataset, eval_tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Q:\\n\\nWhy was Mundungus banned from the Hog's Head?\\n\\nIn Order of the Phoenix while the trio were in the Hogs Head for the first time plotting the start of Dumbledore's Army, it transpires that ol' Dung was lurking in the pub in a disguise, having been banned 20 years previously according to Sirius. \\nFirstly, why was he banned? this could possibly be the tight spot that Albus had helped Dung with in the first place that made him loyal to Albus.  \\nAnd secondly, how is it that he is then speaking to Aberforth in Halfblood Prince? (assuming the ban was for something rather unforgivable, 20 years is a long time?) \\nThey both could have been in the Order by then, but unlikely given Aberforth's attitude in Deathly Hallows once the trio arrive in Hogsmeade looking for the tiara.  We learn now that a lot of trafficking goes on through the Hogs Head so maybe Dung was trading with Aberforth, Sirius' mirror and various other Black artifacts, he just was not allowed in the pub. \\nAnyone with something in canon or more plausible?\\n\\nA:\\n\\nwhy was he banned?\\nI'm not able to find any canon data on that, either book text search or interviews transcripts.\\n\\nhow is it that he is then speaking to Aberforth in Halfblood Prince?\\nIn HBP, he's speaking to Aberforth, NOT being inside Hog's Head. The topic was selling stuff he stole from Sirius' place:\\n\\nNikki: How did sirius twoway mirror end up with aberforth or is it another twoway mirror?\\n  J.K. Rowling: You see Aberforth meeting Mundungus in Hogsmeade. That was the occasion on which Dung, who had taken Sirius’s mirror from Grimmauld Place, sold it to Aberforth.\\n  (src: J.K. Rowling Interview / The Deathly Hallows Web Chat / July 2007)\\n\\nAs a note - this was important since one of the things sold was the 2-way mirror that Harry used to request help when they were imprisoned at Malfoy's in DH.\\nSo, he was banned from the pub (probably, to avoid causing Aberforth's establishment further trouble), but doesn't mean Aberforth won't talk/do business with him otherwise.\\n\\n\",\n",
       " 'meta': {'pile_set_name': 'StackExchange'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datafile['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = load_dataset(\"json\", data_files=data_files, cache_dir='/mnt/share/.cache/huggingface/datasets')\n",
    "# This ran successfully with train/00.jsonl.zst in 27 min I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/mnt/share/.cache/huggingface/datasets/json/default-73de0422a3395cc6/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "100%|██████████| 1/1 [09:43<00:00, 583.41s/it]\n"
     ]
    }
   ],
   "source": [
    "datafile = load_dataset(\"json\", data_files=data_files, cache_dir='/mnt/share/.cache/huggingface/datasets')\n",
    "# Running this with dataset cache\n",
    "# 9:43 to complete.\n",
    "# Multiplied by 30 files: 4 hrs: 50 min just to load the data from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_copy = datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datafile['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_accelerator(cfg: DictConfig) -> Accelerator:\n",
    "    accelerator = (\n",
    "        Accelerator(log_with=cfg.tracking.report_to, project_dir=cfg.output_dir)\n",
    "        if cfg.tracking.enabled\n",
    "        else Accelerator()\n",
    "    )\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    return accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(cfg, accelerator, tokenizer, raw_datasets):\n",
    "\n",
    "    # First we tokenize all the texts.\n",
    "    column_names = raw_datasets.column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[\"train\"][0]\n",
    "    if cfg.dataset.concatenate_raw is True:\n",
    "        pad = False\n",
    "    else:\n",
    "        pad = \"max_length\"\n",
    "\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        if total_length >= cfg.dataset.block_size:\n",
    "            total_length = (\n",
    "                total_length // cfg.dataset.block_size\n",
    "            ) * cfg.dataset.block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [\n",
    "                t[i : i + cfg.dataset.block_size]\n",
    "                for i in range(0, total_length, cfg.dataset.block_size)\n",
    "            ]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        result = tokenizer(\n",
    "            examples[text_column_name],\n",
    "            padding=pad,\n",
    "            truncation=True,\n",
    "            max_length=cfg.dataset.block_size,\n",
    "        )\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_fn,\n",
    "            batched=True,\n",
    "            num_proc=cfg.tokenizer.preprocessing_num_workers,\n",
    "            load_from_cache_file=not cfg.dataset.overwrite_cache,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )\n",
    "\n",
    "        if cfg.dataset.concatenate_raw is True:\n",
    "            tokenized_datasets = tokenized_datasets.map(\n",
    "                group_texts,\n",
    "                batched=True,\n",
    "                num_proc=cfg.tokenizer.preprocessing_num_workers,\n",
    "                load_from_cache_file=not cfg.dataset.overwrite_cache,\n",
    "                desc=f\"Grouping texts in chunks of {cfg.dataset.block_size}\",\n",
    "            )\n",
    "\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('conf/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/repos/clm-finetuning/venv/lib/python3.9/site-packages/accelerate/accelerator.py:359: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n"
     ]
    }
   ],
   "source": [
    "accelerator = create_accelerator(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenized_ds = preprocess(cfg, accelerator, tokenizer, datafile_copy)\n",
    "#2k examples per second\n",
    "# 1:30:45 to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'meta'],\n",
       "        num_rows: 7021438\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'meta', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7021438\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 7021438\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds['train'].remove_columns(['text', 'meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'meta', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 7021438\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_copy = tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds_copy['train'] = tokenized_ds_copy['train'].remove_columns(['text', 'meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7021438\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tokenized_ds_copy.save_to_disk(\"/mnt/share/the_pile/tokenized\")\n",
    "# torch.save(tokenized_ds_copy, \"train_tokenized_00\")\n",
    "# 13:06 to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_ds_copy2 = load_from_disk(\"/mnt/share/the_pile/tokenized\")\n",
    "\n",
    "# How long does this take to run?\n",
    "# This run time was: 6 min 9 seconds. 2nd run was 5m 48sec\n",
    "# This run time is important\n",
    "# Loading in the .zst file was 9 min 45 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6670366\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 351072\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds_copy2['train'].train_test_split(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7021438\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_ds_copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.model.name=='decapoda-research/llama-7b-hf':\n",
    "    tokenized_datasets = tokenized_datasets['train'].train_test_split(\n",
    "        test_size=cfg.training.val_split_percent / 100)\n",
    "else:\n",
    "    if \"train\" not in tokenized_datasets.column_names:\n",
    "        tokenized_datasets = tokenized_datasets['train'].train_test_split(\n",
    "            test_size=cfg.training.val_split_percent / 100)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 5968222\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/repos/clm-finetuning/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
